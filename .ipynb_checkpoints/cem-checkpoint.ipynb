{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" This module implements the Contrastive Explanation Method in Pytorch.\n",
    "\n",
    "Paper:  https://arxiv.org/abs/1802.07623\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class ContrastiveExplanationMethod:\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        classifier,\n",
    "        mode: str,\n",
    "        autoencoder = None,\n",
    "        kappa: float = 0.,\n",
    "        const: float = 10.,\n",
    "        beta: float = .1,\n",
    "        gamma: float = 0.,\n",
    "        feature_range: tuple = (-1e10, 1e10)\n",
    "        iterations: int = 1000,\n",
    "        n_searches: int = 9\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialise the CEM model.\n",
    "        \n",
    "        classifier\n",
    "            classification model to be explained.\n",
    "        mode\n",
    "            for pertinant negatives 'PN' or for pertinant positives 'PP'.\n",
    "        autoencoder\n",
    "            optional, autoencoder to be used for regularisation of the\n",
    "            modifications to the explained samples.\n",
    "        kappa\n",
    "            confidence parameter used in the loss functions (eq. 2) and (eq. 4) in\n",
    "            the original paper.\n",
    "        const\n",
    "            initial regularisation coefficient for the attack loss term.\n",
    "        beta\n",
    "            regularisation coefficent for the L1 term of the optimisation objective.\n",
    "        gamma\n",
    "            regularisation coefficient for the autoencoder term of the optimisation\n",
    "            objective.\n",
    "        feature_range\n",
    "            range over which the features of the perturbed instances should be distributed.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.explain_model = explain_model\n",
    "        self.mode = mode\n",
    "        self.autoencoder = autoencoder\n",
    "        self.kappa = kappa\n",
    "        self.const = const\n",
    "        self.beta = beta\n",
    "        self.gamma = gamma\n",
    "        self.feature_range = feature_range\n",
    "        self.iterations = iterations\n",
    "        self.n_searches = n_searches\n",
    "        \n",
    "        self.delta = torch.zeros(orig_sample.shape)\n",
    "        self.y = torch.zeros(orig_sample.shape)\n",
    "        \n",
    "        self.prev_deltas = []\n",
    "    \n",
    "    def fista(self, orig_sample):\n",
    "        \"\"\"Fast Iterative Shrinkage Thresholding Algorithm implementation in pytorch\n",
    "        \n",
    "        Paper: https://doi.org/10.1137/080716542\n",
    "        \n",
    "        (Eq. 5) and (eq. 6) in https://arxiv.org/abs/1802.07623\n",
    "        \"\"\"\n",
    "\n",
    "        if self.mode == \"PP\":\n",
    "            delta_space = orig_sample.copy()\n",
    "        elif self.mode == \"PN\":\n",
    "            delta_space = torch.ones(orig_sample.shape) - orig_sample\n",
    "        \n",
    "        while stopping_condition:\n",
    "            \n",
    "            # See appendix A\n",
    "            for _ in range(self.n_searches):\n",
    "                for i in range(1, self.iterations + 1):\n",
    "\n",
    "                    # TODO: vector projection onto where?\n",
    "\n",
    "                    # BATCH SIZE?\n",
    "                    obj = (self.optimisation_obj(orig_sample)).sum()\n",
    "                    obj.backward()\n",
    "                    self.prev_deltas.append(self.delta.copy().detach())\n",
    "\n",
    "                    self.delta = self.shrink(self.y - self.learning_rate * self.y.grad)\n",
    "                    self.y = (self.delta + i/(i + 3)(self.delta - self.prev_deltas[-1]))\n",
    "\n",
    "                    \n",
    "    def shrink(self, z):\n",
    "        \"\"\"Element-wise shrinkage thresholding function.\n",
    "        \n",
    "        (Eq. 7) in https://arxiv.org/abs/1802.07623\n",
    "        \"\"\"\n",
    "        zeros = torch.zeros(z.shape)\n",
    "        z_min = z - self.beta\n",
    "        z_plus = z + self.beta\n",
    "        \n",
    "        z_shrunk = z.copy()\n",
    "        z_shrunk = torch.where(torch.abs(z) <= self.beta, zeros, z_shrunk)\n",
    "        z_shrunk = torch.where(z > self.beta, z_min, z_shrunk)\n",
    "        z_shrunk = torch.where(z < -self.beta, z_plus, z_shrunk)\n",
    "        return z_shrunk\n",
    "                        \n",
    "    def optimisation_obj(self, orig_sample):\n",
    "        \"\"\"\n",
    "        Optimisation objective for PN (eq. 1) and for PP (eq. 3).\n",
    "        \"\"\"\n",
    "        \n",
    "        obj = (\n",
    "            self.const * self.loss_fn(orig_sample) +\n",
    "            self.beta * torch.sum(torch.abs(self.y)) + # IS DIT GOED?\n",
    "            torch.norm(self.y) ** 2\n",
    "        )\n",
    "        if callable(self.autoencoder):\n",
    "            if self.mode == \"PN\":\n",
    "                obj + gamma * torch.norm(orig_sample + self.y - self.autoencoder(orig_sample + self.y), axis=1) ** 2\n",
    "            elif self.mode == \"PP\":\n",
    "                obj + gamma * torch.norm(self.y - self.autoencoder(self.y), axis=1) ** 2\n",
    "        return obj\n",
    "\n",
    "    def loss_fn(self, orig_sample):\n",
    "        \"\"\"\n",
    "        Loss term f(x,d) for PN (eq. 2) and for PP (eq. 4).\n",
    "        \n",
    "        orig_sample\n",
    "            the unperturbed original sample, batch size first.\n",
    "        \"\"\"\n",
    "        \n",
    "        orig_output = self.classifier(orig_sample)\n",
    "        target_mask = torch.zeros(orig_output.shape)\n",
    "        target_mask[torch.arange(orig_output.shape[0]), torch.argmax(orig_output, axis=1)] = 1\n",
    "        nontarget_mask = torch.ones(orig_output.shape) - target_mask\n",
    "        \n",
    "        if self.mode == \"PN\":\n",
    "            pert_output = torch.max(nontarget_mask * self.classifier(orig_sample + self.y), axis=1)\n",
    "            perturbation_loss = torch.max(\n",
    "                torch.max(orig_output, axis=1) - pert_output,\n",
    "                -self.kappa\n",
    "            )\n",
    "        elif self.mode == \"PP\":\n",
    "            pert_output = self.classifier(self.y)\n",
    "            perturbation_loss = torch.max(\n",
    "                torch.max(nontarget_mask * pert_output, axis=1) - torch.max(target_mask * pert_output, axis=1),\n",
    "                -self.kappa\n",
    "            )\n",
    "        \n",
    "        return perturbation_loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
