{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import os\n",
    "import torch\n",
    "\n",
    "from datasets.mnist import MNIST\n",
    "\n",
    "from models.cae_model import CAE\n",
    "from models.conv_model import CNN\n",
    "\n",
    "from train import train_ae, train_cnn\n",
    "\n",
    "from cem import ContrastiveExplanationMethod as ContrastiveExplanationMethod\n",
    "\n",
    "# set random seeds for reproducability (although the CEM is fully determininstic)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the following variables to either include or skip over optional\n",
    "# evaluation metrics for the CNN and autoencoder\n",
    "EVALUATE_CNN = False\n",
    "EVALUATE_AUTOENCODER = False\n",
    "\n",
    "# Change the following variable to have the script create full set of images\n",
    "# for every class, used in the paper.\n",
    "EVALUATE_CEM = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of the CEM on the MNIST dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MNIST(batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the classifier\n",
    "\n",
    "This section trains the classifier for which the pertinent positives and negatives will be constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN(device=\"cpu\")\n",
    "\n",
    "train_cnn(cnn, dataset, iterations=20, lr=0.01, save_fn='mnist-cnn', device=\"cpu\", load_path=\"models/saved_models/mnist-cnn.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\david\\Coding\\uva-fact-ai\\models\\conv_model.py:50: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  out = nn.functional.softmax(out)\n"
     ]
    }
   ],
   "source": [
    "images, _ = dataset.get_batch()\n",
    "\n",
    "output = cnn(images)\n",
    "\n",
    "images = images.numpy()\n",
    "output = output.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell evaluates the performance of the CNN on the specified test set.\n",
    "if EVALUATE_CNN:\n",
    "\n",
    "    total_acc = 0\n",
    "    total_batches = 0\n",
    "    for step, (batch_inputs, batch_targets) in enumerate(dataset.test_loader):\n",
    "\n",
    "        predictions = cnn(batch_inputs)\n",
    "        acc = (predictions.argmax(1).cpu().numpy() == batch_targets.cpu().numpy()).sum()/(predictions.shape[0] )\n",
    "        total_batches += 1\n",
    "        total_acc += acc\n",
    "\n",
    "    print(\"acc: {}\".format(total_acc / total_batches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section trains the autoencoder which will be used as regularizer for the data space which the perturbations are found in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train or load autoencoder\n",
    "cae = CAE(device=\"cpu\")\n",
    "\n",
    "train_ae(cae, dataset, iterations=10, save_fn=\"mnist-cae\", device=\"cpu\", load_path=\"models/saved_models/mnist-cae-no-rs.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell evaluates the performance of the autoencoder by visualising a batch of \n",
    "# in- and corresponding output images.\n",
    "if EVALUATE_AUTOENCODER:\n",
    "    images, _ = dataset.get_batch()\n",
    "    images += 0.5 # rescale images\n",
    "\n",
    "    # get sample outputs\n",
    "    output = cae(images)\n",
    "    # prep images for display\n",
    "    images = images.numpy()\n",
    "    output = output.detach().numpy()\n",
    "\n",
    "    # plot the first ten input images and then reconstructed images\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=4, sharex=True, sharey=True, figsize=(12,4))\n",
    "\n",
    "    # input images on top row, reconstructions on bottom\n",
    "    for images, row in zip([images, output], axes):\n",
    "        for img, ax in zip(images, row):\n",
    "            ax.imshow(np.squeeze(img))\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contrastive Explanation Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal: kappa 30, gamma 1.0, beta 0.1, c 0.1, lr 0.01\n",
    "\n",
    "kappa = 30\n",
    "gamma = 1.0\n",
    "beta = 0.1\n",
    "c = 0.1\n",
    "lr = 0.01\n",
    "\n",
    "CEM = ContrastiveExplanationMethod(\n",
    "    cnn,\n",
    "    cae,\n",
    "    iterations=1000,\n",
    "    n_searches=9,\n",
    "    kappa=kappa,\n",
    "    gamma=gamma,\n",
    "    beta=beta,\n",
    "    learning_rate=lr,\n",
    "    c_init=10.0,\n",
    "    c_converge=c,\n",
    "    verbal=True,\n",
    "    print_every=500\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_save_pertinents(image):\n",
    "    \"\"\"\n",
    "    Finds pertinent positives and negatives for a given image,\n",
    "    creates and saves visualisations of the original image, the pertinents and\n",
    "    the image after being perturbed by the pertinent.\n",
    "    \n",
    "    image\n",
    "        The image to find pertinent positives and negatives for.\n",
    "    \"\"\"\n",
    "    before = np.argmax(cnn(image.squeeze(-1)).detach()).item()\n",
    "    \n",
    "    print(\"FINDING PERTINENTS FOR IMAGE OF CLASS: {}\\n\\n\".format(before))\n",
    "    \n",
    "    for mode in [\"PP\", \"PN\"]:\n",
    "        \n",
    "        CEM.fista(image, mode=mode)\n",
    "        \n",
    "        if mode == \"PP\":\n",
    "            after = np.argmax(cnn(image.squeeze(-1) - CEM.best_delta.view(1,28,28)).detach()).item()\n",
    "        else:\n",
    "            after = np.argmax(cnn(CEM.best_delta.view(1,28,28)).detach()).item()\n",
    "            \n",
    "        dirname = \"saved_perturbations/mode-{}-kappa-{}-gamma-{}-beta-{}-lr-{}\".format(mode, kappa, gamma, beta, lr)\n",
    "        os.makedirs(dirname, exist_ok=True)\n",
    "\n",
    "        fname_orig = dirname + \"/{}-orig.png\".format(before)\n",
    "        fname_pert = dirname + \"/before-{}-after-{}-pert.png\".format(before, after)\n",
    "        fname_combined_pn = dirname + \"/before-{}-after-{}-pn.png\".format(before, after)\n",
    "        fname_combined_pp = dirname + \"/before-{}-after-{}-pp.png\".format(before, after)\n",
    "\n",
    "        if mode == \"PP\":\n",
    "            plt.imsave(fname_orig, image.squeeze(), cmap=\"gray\")\n",
    "            plt.imsave(fname_pert, CEM.best_delta.view(28,28) - image.squeeze(), cmap=\"gray\")\n",
    "            plt.imsave(fname_combined_pp, image.squeeze() - CEM.best_delta.view(28,28), cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imsave(fname_orig, image.squeeze(), cmap=\"gray\")\n",
    "            plt.imsave(fname_pert, CEM.best_delta.view(28,28) - image.squeeze(), cmap=\"gray\")\n",
    "            plt.imsave(fname_combined, CEM.best_delta.view(28,28), cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This cell samples an image from every class and creates its pertinent positives and\n",
    "# negatives. Running this cell creates the images used in the paper.\n",
    "if EVALUATE_CEM:\n",
    "    for i in range(10):\n",
    "\n",
    "        # obtain one sample from each class\n",
    "        image = dataset.get_sample_by_class(class_label=i, show_image=False)\n",
    "        find_save_pertinents(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one sample\n",
    "image, _ = dataset.get_sample(show_image=False)\n",
    "before = np.argmax(cnn(image.squeeze(-1)).detach()).item()\n",
    "\n",
    "mode = \"PP\"\n",
    "best_delta = CEM.explain(image, mode=mode)\n",
    "\n",
    "if mode == \"PP\":\n",
    "    after = np.argmax(cnn(image.squeeze(-1) - best_delta.view(1,28,28)).detach()).item()\n",
    "else:\n",
    "    after = np.argmax(cnn(best_delta.view(-1,1,28,28)).detach()).item()\n",
    "        \n",
    "plt.imshow(image.squeeze(), cmap=\"gray\")\n",
    "plt.title(\"original image, classified as: {}\".format(before))\n",
    "plt.show()\n",
    "\n",
    "if mode == \"PP\":\n",
    "    plt.imshow(image.squeeze() - best_delta.view(28, 28))\n",
    "    plt.title(\"pertinent positive, classified as: {}\".format(after))\n",
    "    plt.show()\n",
    "else:\n",
    "    plt.imshow(best_delta.squeeze())\n",
    "    plt.title(\"image with pertinent negative, classified as: {}\".format(after))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of the CEM on the FashionMNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FashionMNIST(batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train or load autoencoder\n",
    "cae = CAE(device=\"cpu\")\n",
    "\n",
    "train_ae(cae, dataset, iterations=10, save_fn=\"fashion-mnist-cae\", device=\"cpu\", load_path=\"models/saved_models/fashion-mnist-cae.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
